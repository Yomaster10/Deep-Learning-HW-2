{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "# Part 1: Backpropagation\n",
    "<a id=part1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this part we will learn about **backpropagation** and **automatic differentiation**. We'll implement both of these concepts from scratch and compare our implementation to `PyTorch`'s built in implementation (`autograd`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T21:11:00.383385Z",
     "iopub.status.busy": "2022-04-28T21:11:00.383059Z",
     "iopub.status.idle": "2022-04-28T21:11:01.533749Z",
     "shell.execute_reply": "2022-04-28T21:11:01.533434Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import unittest\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "test = unittest.TestCase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The backpropagation algorithm is at the core of training deep models. To state the problem we'll tackle in this notebook, imagine we have an L-layer MLP model, defined as\n",
    "$$\n",
    "\\hat{\\vec{y}^i} = \\vec{y}_L^i= \\varphi_L \\left(\n",
    "\\mat{W}_L \\varphi_{L-1} \\left( \\cdots\n",
    "\\varphi_1 \\left( \\mat{W}_1 \\vec{x}^i + \\vec{b}_1 \\right)\n",
    "\\cdots \\right)\n",
    "+ \\vec{b}_L \\right),\n",
    "$$\n",
    "\n",
    "a pointwise loss function $\\ell(\\vec{y}, \\hat{\\vec{y}})$ and an empirical loss over our entire data set,\n",
    "$$\n",
    "L(\\vec{\\theta}) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell(\\vec{y}^i, \\hat{\\vec{y}^i}) + R(\\vec{\\theta})\n",
    "$$\n",
    "\n",
    "where $\\vec{\\theta}$ is a vector containing all network parameters, e.g.\n",
    "$\\vec{\\theta} = \\left[ \\mat{W}_{1,:}, \\vec{b}_1, \\dots,  \\mat{W}_{L,:}, \\vec{b}_L \\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In order to train our model we would like to calculate the derivative\n",
    "(or **gradient**, in the multivariate case) of the loss with respect to each and every one of the parameters,\n",
    "i.e. $\\pderiv{L}{\\mat{W}_j}$ and $\\pderiv{L}{\\vec{b}_j}$ for all $j$.\n",
    "Since the gradient \"points\" to the direction of functional increase, the negative gradient is often used as a descent direction for descent-based optimization algorithms.\n",
    "In other words, iteratively updating each parameter proportianally to it's negetive gradient can lead to\n",
    "convergence to a local minimum of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Calculus tells us that as long as we know the derivatives of all the functions \"along the way\"\n",
    "($\\varphi_i(\\cdot),\\ \\ell(\\cdot,\\cdot),\\ R(\\cdot)$)\n",
    "we can use the **chain rule** to calculate the derivative \n",
    "of the loss with respect to any one of the parameter vectors.\n",
    "Note that if the loss $L(\\vec{\\theta})$ is scalar (which is usually the case), the gradient of a parameter\n",
    "will have the same shape as the parameter itself (matrix/vector/tensor of same dimensions).\n",
    "\n",
    "For deep models that are a composition of many functions, calculating the gradient of each parameter by hand and implementing hard-coded gradient derivations quickly becomes infeasible.\n",
    "Additionally, such code makes models hard to change, since any change potentially requires re-derivation and re-implementation of the entire gradient function.\n",
    "\n",
    "The backpropagation algorithm, which we saw [in the lecture](https://vistalab-technion.github.io/cs236781/lecture_notes/lecture_03/#error-backpropagation), provides us with a effective method of applying the **chain rule** recursively so that we can implement gradient calculations of arbitrarily deep or complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We'll now implement backpropagation using a modular approach, which will allow us to chain many components layers together and get automatic gradient calculation of the output with respect to the input or any intermediate parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To do this, we'll define a `Layer` class. Here's the API of this class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T21:11:01.536758Z",
     "iopub.status.busy": "2022-04-28T21:11:01.536642Z",
     "iopub.status.idle": "2022-04-28T21:11:01.553933Z",
     "shell.execute_reply": "2022-04-28T21:11:01.553674Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hw2.layers as layers\n",
    "help(layers.Layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In other words, a `Layer` can be anything: a layer, an activation function, a loss function or generally *any computation that we know how to derive a gradient for*.\n",
    "\n",
    "Each `Layer` must define a `forward()` function and a `backward()` function.\n",
    "- The `forward()` function performs the actual calculation/operation of the block and returns an output.\n",
    "- The `backward()` function computes the gradient of the **input and parameters** as a function of the gradient of the **output**, according to the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here's a diagram illustrating the above explanation:\n",
    "\n",
    "<img src=\"imgs/backprop.png\" width=\"900\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that the diagram doesn't show that if the function is parametrized, i.e. $f(\\vec{x},\\vec{y})=f(\\vec{x},\\vec{y};\\vec{w})$, there are also gradients to calculate for the parameters $\\vec{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The forward pass is straightforward: just do the computation.\n",
    "To understand the backward pass, imagine that there's some \"downstream\" loss function\n",
    "$L(\\vec{\\theta})$ and magically somehow we are told the gradient of that loss with respect\n",
    "to the **output** $\\vec{z}$ of our block, i.e. $\\pderiv{L}{\\vec{z}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, since we know how to calculate the derivative of $f(\\vec{x},\\vec{y};\\vec{w})$,\n",
    "it means we know how to calculate $\\pderiv{\\vec{z}}{\\vec{x}}$, $\\pderiv{\\vec{z}}{\\vec{y}}$ and $\\pderiv{\\vec{z}}{\\vec{w}}$ .\n",
    "Thanks to the chain rule, this is all we need to calculate the gradients of the **loss** w.r.t. the input and\n",
    "parameters:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\pderiv{L}{\\vec{x}} &= \\pderiv{L}{\\vec{z}}\\cdot \\pderiv{\\vec{z}}{\\vec{x}}\\\\\n",
    "\\pderiv{L}{\\vec{y}} &= \\pderiv{L}{\\vec{z}}\\cdot \\pderiv{\\vec{z}}{\\vec{y}}\\\\\n",
    "\\pderiv{L}{\\vec{w}} &= \\pderiv{L}{\\vec{z}}\\cdot \\pderiv{\\vec{z}}{\\vec{w}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Comparison with PyTorch\n",
    "<a id=part1_1></a>\n",
    "\n",
    "PyTorch has the [`nn.Module`](https://pytorch.org/docs/stable/nn.html#module) base class, which may seem to be similar to our `Layer` since it also represents a computation element in a network.\n",
    "However PyTorch's `nn.Module`s don't compute the gradient directly, they only define the forward calculations.\n",
    "Instead, PyTorch has a more low-level API for defining a function and explicitly implementing it's `forward()` and `backward()`. See [`autograd.Function`](https://pytorch.org/docs/stable/autograd.html#function).\n",
    "When an operation is performed on a tensor, it creates a `Function` instance which performs the operation and\n",
    "stores any necessary information for calculating the gradient later on. Additionally, `Functions`s point to the \n",
    "other `Function` objects representing the operations performed earlier on the tensor. Thus, a graph (or DAG)\n",
    "of operations is created (this is not 100% exact, as the graph is actually composed of a different type of class which wraps the backward method, but it's accurate enough for our purposes).\n",
    "\n",
    "A `Tensor` instance which was created by performing operations on one or more tensors with `requires_grad=True`, has a `grad_fn` property which is a `Function` instance representing the last operation performed to produce this tensor.\n",
    "This exposes the graph of `Function` instances, each with it's own `backward()` function. Therefore, in PyTorch the `backward()` function is called on the tensors, not the modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our `Layer`s are therefore a combination of the ideas in `Module` and `Function` and we'll implement them together,\n",
    "just to make things simpler.\n",
    "Our goal here is to create a \"poor man's autograd\": We'll use PyTorch tensors,\n",
    "but we'll calculate and store the gradients in our `Layer`s (or return them).\n",
    "The gradients we'll calculate are of the entire block, not individual operations on tensors.\n",
    "\n",
    "To test our implementation, we'll use PyTorch's `autograd`.\n",
    "\n",
    "Note that of course this method of tracking gradients is **much** more limited than what PyTorch offers. However it allows us to implement the backpropagation algorithm very simply and really see how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's set up some testing instrumentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T21:11:01.555992Z",
     "iopub.status.busy": "2022-04-28T21:11:01.555887Z",
     "iopub.status.idle": "2022-04-28T21:11:01.605067Z",
     "shell.execute_reply": "2022-04-28T21:11:01.604738Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hw2.grad_compare import compare_layer_to_torch\n",
    "\n",
    "def test_block_grad(block: layers.Layer, x, y=None, delta=1e-3):\n",
    "    diffs = compare_layer_to_torch(block, x, y)\n",
    "    \n",
    "    # Assert diff values\n",
    "    for diff in diffs:\n",
    "        test.assertLess(diff, delta)\n",
    "\n",
    "# Show the compare function\n",
    "compare_layer_to_torch??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Notes:\n",
    "- After you complete your implementation, you should make sure to read and understand the `compare_layer_to_torch()` function. It will help you understand what PyTorch is doing.\n",
    "- The value of `delta` above is should not be needed. A correct implementation will give you a `diff` of exactly zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Layer Implementations\n",
    "<a id=part1_2></a>\n",
    "\n",
    "We'll now implement some `Layer`s that will enable us to later build an MLP model of arbitrary depth, complete with automatic differentiation.\n",
    "\n",
    "For each block, you'll first implement the `forward()` function.\n",
    "Then, you will calculate the derivative of the block by hand with respect to each of its\n",
    "input tensors and each of its parameter tensors (if any).\n",
    "Using your manually-calculated derivation, you can then implement the `backward()` function.\n",
    "\n",
    "Notice that we have intermediate Jacobians that are potentially high dimensional tensors.\n",
    "For example in the expression\n",
    "$\\pderiv{L}{\\vec{w}} = \\pderiv{L}{\\vec{z}}\\cdot \\pderiv{\\vec{z}}{\\vec{w}}$,\n",
    "the term $\\pderiv{\\vec{z}}{\\vec{w}}$ is a 4D Jacobian if both $\\vec{z}$ and $\\vec{w}$\n",
    "are 2D matrices.\n",
    "\n",
    "In order to implement the backpropagation algorithm efficiently,\n",
    "we need to implement every backward function without explicitly constructing this \n",
    "Jacobian. Instead, we're interested in directly calculating the vector-Jacobian product\n",
    "(VJP) $\\pderiv{L}{\\vec{z}}\\cdot \\pderiv{\\vec{z}}{\\vec{w}}$. \n",
    "In order to do this, you should try to figure out the gradient of the loss with respect to\n",
    "one element, e.g. $\\pderiv{L}{\\vec{w}_{1,1}}$ and extrapolate from there how to\n",
    "directly obtain the VJP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (Leaky) ReLU\n",
    "\n",
    "ReLU, or rectified linear unit is a very common activation function in deep learning architectures.\n",
    "In it's most standard form, as we'll implement here, it has no parameters.\n",
    "\n",
    "We'll first implement the \"leaky\" version, defined as\n",
    "\n",
    "$$\n",
    "\\mathrm{relu}(\\vec{x}) = \\max(\\alpha\\vec{x},\\vec{x}), \\ 0\\leq\\alpha<1\n",
    "$$\n",
    "\n",
    "This is similar to the ReLU activation we've seen in class, only that it has a small non-zero slope then it's input is negative.\n",
    "Note that it's not strictly differentiable, however it has sub-gradients, defined separately any positive-valued input and for negative-valued input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO**: Complete the implementation of the `LeakyReLU` class in the `hw2/layers.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T21:11:01.607221Z",
     "iopub.status.busy": "2022-04-28T21:11:01.607114Z",
     "iopub.status.idle": "2022-04-28T21:11:01.620715Z",
     "shell.execute_reply": "2022-04-28T21:11:01.620450Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "in_features = 200\n",
    "num_classes = 10\n",
    "eps = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T21:11:01.622385Z",
     "iopub.status.busy": "2022-04-28T21:11:01.622305Z",
     "iopub.status.idle": "2022-04-28T21:11:01.641285Z",
     "shell.execute_reply": "2022-04-28T21:11:01.641015Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test LeakyReLU\n",
    "alpha = 0.1\n",
    "lrelu = layers.LeakyReLU(alpha=alpha)\n",
    "x_test = torch.randn(N, in_features)\n",
    "\n",
    "# Test forward pass\n",
    "z = lrelu(x_test)\n",
    "test.assertSequenceEqual(z.shape, x_test.shape)\n",
    "test.assertTrue(torch.allclose(z, torch.nn.LeakyReLU(alpha)(x_test), atol=eps))\n",
    "\n",
    "# Test backward pass\n",
    "test_block_grad(lrelu, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now using the LeakyReLU, we can trivially define a regular ReLU block as a special case.\n",
    "\n",
    "**TODO**: Complete the implementation of the `ReLU` class in the `hw2/layers.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T21:11:01.643060Z",
     "iopub.status.busy": "2022-04-28T21:11:01.642954Z",
     "iopub.status.idle": "2022-04-28T21:11:01.659264Z",
     "shell.execute_reply": "2022-04-28T21:11:01.658980Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test ReLU\n",
    "relu = layers.ReLU()\n",
    "x_test = torch.randn(N, in_features)\n",
    "\n",
    "# Test forward pass\n",
    "z = relu(x_test)\n",
    "test.assertSequenceEqual(z.shape, x_test.shape)\n",
    "test.assertTrue(torch.allclose(z, torch.relu(x_test), atol=eps))\n",
    "\n",
    "# Test backward pass\n",
    "test_block_grad(relu, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Sigmoid\n",
    "\n",
    "The sigmoid function $\\sigma(x)$ is also sometimes used as an activation function.\n",
    "We have also seen it previously in the context of logistic regression.\n",
    "\n",
    "The sigmoid function is defined as\n",
    "\n",
    "$$\n",
    "\\sigma(\\vec{x}) = \\frac{1}{1+\\exp(-\\vec{x})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T21:11:01.661131Z",
     "iopub.status.busy": "2022-04-28T21:11:01.661028Z",
     "iopub.status.idle": "2022-04-28T21:11:01.891792Z",
     "shell.execute_reply": "2022-04-28T21:11:01.891479Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Sigmoid\n",
    "sigmoid = layers.Sigmoid()\n",
    "x_test = torch.randn(N, in_features, in_features) # 3D input should work\n",
    "\n",
    "# Test forward pass\n",
    "z = sigmoid(x_test)\n",
    "test.assertSequenceEqual(z.shape, x_test.shape)\n",
    "test.assertTrue(torch.allclose(z, torch.sigmoid(x_test), atol=eps))\n",
    "\n",
    "# Test backward pass\n",
    "test_block_grad(sigmoid, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Hyperbolic Tangent\n",
    "\n",
    "The hyperbolic tangent function $\\tanh(x)$ is a common activation function used when the output should be in the range \\[-1, 1\\].\n",
    "\n",
    "The tanh function is defined as\n",
    "\n",
    "$$\n",
    "\\tanh(\\vec{x}) = \\frac{\\exp(x)-\\exp(-x)}{\\exp(x)+\\exp(-\\vec{x})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T21:11:01.894010Z",
     "iopub.status.busy": "2022-04-28T21:11:01.893884Z",
     "iopub.status.idle": "2022-04-28T21:11:02.140587Z",
     "shell.execute_reply": "2022-04-28T21:11:02.140230Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test TanH\n",
    "tanh = layers.TanH()\n",
    "x_test = torch.randn(N, in_features, in_features) # 3D input should work\n",
    "\n",
    "# Test forward pass\n",
    "z = tanh(x_test)\n",
    "test.assertSequenceEqual(z.shape, x_test.shape)\n",
    "test.assertTrue(torch.allclose(z, torch.tanh(x_test), atol=eps))\n",
    "\n",
    "# Test backward pass\n",
    "test_block_grad(tanh, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Linear (fully connected) layer\n",
    "\n",
    "First, we'll implement an affine transform layer, also known as a fully connected layer.\n",
    "\n",
    "Given an input $\\mat{X}$ the layer computes,\n",
    "\n",
    "$$\n",
    "\\mat{Z} = \\mat{X} \\mattr{W}  + \\vec{b} ,~\n",
    "\\mat{X}\\in\\set{R}^{N\\times D_{\\mathrm{in}}},~\n",
    "\\mat{W}\\in\\set{R}^{D_{\\mathrm{out}}\\times D_{\\mathrm{in}}},~ \\vec{b}\\in\\set{R}^{D_{\\mathrm{out}}}.\n",
    "$$\n",
    "\n",
    "Notes:\n",
    "- We write it this way to follow the implementation conventions.\n",
    "- $N$ is the number of samples in the input (batch size). The input $\\mat{X}$ will always be a tensor containing a batch dimension first.\n",
    "- Thanks to broadcasting, $\\vec{b}$ can remain a vector even though the input $\\mat{X}$ is a matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO**: Complete the implementation of the `Linear` class in the `hw2/layers.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T21:11:02.142641Z",
     "iopub.status.busy": "2022-04-28T21:11:02.142527Z",
     "iopub.status.idle": "2022-04-28T21:11:02.199831Z",
     "shell.execute_reply": "2022-04-28T21:11:02.199509Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Linear\n",
    "out_features = 1000\n",
    "fc = layers.Linear(in_features, out_features)\n",
    "x_test = torch.randn(N, in_features)\n",
    "\n",
    "# Test forward pass\n",
    "z = fc(x_test)\n",
    "test.assertSequenceEqual(z.shape, [N, out_features])\n",
    "torch_fc = torch.nn.Linear(in_features, out_features,bias=True)\n",
    "torch_fc.weight = torch.nn.Parameter(fc.w)\n",
    "torch_fc.bias = torch.nn.Parameter(fc.b)\n",
    "test.assertTrue(torch.allclose(torch_fc(x_test), z, atol=eps))\n",
    "\n",
    "# Test backward pass\n",
    "test_block_grad(fc, x_test)\n",
    "\n",
    "# Test second backward pass\n",
    "x_test = torch.randn(N, in_features)\n",
    "z = fc(x_test)\n",
    "z = fc(x_test)\n",
    "test_block_grad(fc, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cross-Entropy Loss\n",
    "\n",
    "As you know by know, cross-entropy is a common loss function for classification tasks.\n",
    "In class, we defined it as \n",
    "\n",
    "$$\\ell_{\\mathrm{CE}}(\\vec{y},\\hat{\\vec{y}}) = - {\\vectr{y}} \\log(\\hat{\\vec{y}})$$\n",
    "\n",
    "where $\\hat{\\vec{y}} = \\mathrm{softmax}(x)$ is a probability vector (the output of softmax on the class scores $\\vec{x}$) and the vector $\\vec{y}$ is a 1-hot encoded class label.\n",
    "\n",
    "However, it's tricky to compute the gradient of softmax, so instead we'll define a version of cross-entropy that produces the exact same output but works directly on the class scores $\\vec{x}$.\n",
    "\n",
    "We can write,\n",
    "$$\\begin{align}\n",
    "\\ell_{\\mathrm{CE}}(\\vec{y},\\hat{\\vec{y}}) &= - {\\vectr{y}} \\log(\\hat{\\vec{y}}) \n",
    "= - {\\vectr{y}} \\log\\left(\\mathrm{softmax}(\\vec{x})\\right) \\\\\n",
    "&= - {\\vectr{y}} \\log\\left(\\frac{e^{\\vec{x}}}{\\sum_k e^{x_k}}\\right) \\\\\n",
    "&= - \\log\\left(\\frac{e^{x_y}}{\\sum_k e^{x_k}}\\right) \\\\\n",
    "&= - \\left(\\log\\left(e^{x_y}\\right) - \\log\\left(\\sum_k e^{x_k}\\right)\\right)\\\\\n",
    "&= - x_y + \\log\\left(\\sum_k e^{x_k}\\right)\n",
    "\\end{align}$$\n",
    "\n",
    "Where the scalar $y$ is the correct class label, so $x_y$ is the correct class score.\n",
    "\n",
    "Note that this version of cross entropy is also what's [provided](https://pytorch.org/docs/stable/nn.html#crossentropyloss) by PyTorch's `nn` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO**: Complete the implementation of the `CrossEntropyLoss` class in the `hw2/layers.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T21:11:02.201807Z",
     "iopub.status.busy": "2022-04-28T21:11:02.201697Z",
     "iopub.status.idle": "2022-04-28T21:11:02.221055Z",
     "shell.execute_reply": "2022-04-28T21:11:02.220777Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test CrossEntropy\n",
    "cross_entropy = layers.CrossEntropyLoss()\n",
    "scores = torch.randn(N, num_classes)\n",
    "labels = torch.randint(low=0, high=num_classes, size=(N,), dtype=torch.long)\n",
    "\n",
    "# Test forward pass\n",
    "loss = cross_entropy(scores, labels)\n",
    "expected_loss = torch.nn.functional.cross_entropy(scores, labels)\n",
    "test.assertLess(torch.abs(expected_loss-loss).item(), 1e-5)\n",
    "print('loss=', loss.item())\n",
    "\n",
    "# Test backward pass\n",
    "test_block_grad(cross_entropy, scores, y=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Building Models\n",
    "<a id=part1_3></a>\n",
    "\n",
    "Now that we have some working `Layer`s, we can build an MLP model of arbitrary depth and compute end-to-end gradients.\n",
    "\n",
    "First, lets copy an idea from PyTorch and implement our own version of the `nn.Sequential` `Module`.\n",
    "This is a `Layer` which contains other `Layer`s and calls them in sequence. We'll use this to build our MLP model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO**: Complete the implementation of the `Sequential` class in the `hw2/layers.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T21:11:02.222918Z",
     "iopub.status.busy": "2022-04-28T21:11:02.222806Z",
     "iopub.status.idle": "2022-04-28T21:11:02.280845Z",
     "shell.execute_reply": "2022-04-28T21:11:02.280515Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Sequential\n",
    "# Let's create a long sequence of layers and see\n",
    "# whether we can compute end-to-end gradients of the whole thing.\n",
    "\n",
    "seq = layers.Sequential(\n",
    "    layers.Linear(in_features, 100),\n",
    "    layers.Linear(100, 200),\n",
    "    layers.Linear(200, 100),\n",
    "    layers.ReLU(),\n",
    "    layers.Linear(100, 500),\n",
    "    layers.LeakyReLU(alpha=0.01),\n",
    "    layers.Linear(500, 200),\n",
    "    layers.ReLU(),\n",
    "    layers.Linear(200, 500),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.Linear(500, 1),\n",
    "    layers.Sigmoid(),\n",
    ")\n",
    "x_test = torch.randn(N, in_features)\n",
    "\n",
    "# Test forward pass\n",
    "z = seq(x_test)\n",
    "test.assertSequenceEqual(z.shape, [N, 1])\n",
    "\n",
    "# Test backward pass\n",
    "test_block_grad(seq, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, equipped with a `Sequential`, all we have to do is create an MLP architecture.\n",
    "We'll define our MLP with the following hyperparameters:\n",
    "- Number of input features, $D$.\n",
    "- Number of output classes, $C$.\n",
    "- Sizes of hidden layers, $h_1,\\dots,h_L$.\n",
    "\n",
    "So the architecture will be:\n",
    "\n",
    "FC($D$, $h_1$) $\\rightarrow$ ReLU $\\rightarrow$\n",
    "FC($h_1$, $h_2$) $\\rightarrow$ ReLU $\\rightarrow$\n",
    "$\\cdots$ $\\rightarrow$\n",
    "FC($h_{L-1}$, $h_L$) $\\rightarrow$ ReLU $\\rightarrow$\n",
    "FC($h_{L}$, $C$)\n",
    "\n",
    "We'll also create a sequence of the above MLP and a cross-entropy loss, since it's the gradient of the loss that we need in order to train a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO**: Complete the implementation of the `MLP` class in the `hw2/layers.py` module. Ignore the `dropout` parameter for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T21:11:02.282850Z",
     "iopub.status.busy": "2022-04-28T21:11:02.282766Z",
     "iopub.status.idle": "2022-04-28T21:11:02.297662Z",
     "shell.execute_reply": "2022-04-28T21:11:02.297404Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an MLP model\n",
    "mlp = layers.MLP(in_features, num_classes, hidden_features=[100, 50, 100])\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T21:11:02.299380Z",
     "iopub.status.busy": "2022-04-28T21:11:02.299271Z",
     "iopub.status.idle": "2022-04-28T21:11:02.338788Z",
     "shell.execute_reply": "2022-04-28T21:11:02.338503Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test MLP architecture\n",
    "N = 100\n",
    "in_features = 10\n",
    "num_classes = 10\n",
    "for activation in ('relu', 'sigmoid'):\n",
    "    mlp = layers.MLP(in_features, num_classes, hidden_features=[100, 50, 100], activation=activation)\n",
    "    test.assertEqual(len(mlp.sequence), 7)\n",
    "    \n",
    "    num_linear = 0\n",
    "    for b1, b2 in zip(mlp.sequence, mlp.sequence[1:]):\n",
    "        if (str(b2).lower() == activation):\n",
    "            test.assertTrue(str(b1).startswith('Linear'))\n",
    "            num_linear += 1\n",
    "            \n",
    "    test.assertTrue(str(mlp.sequence[-1]).startswith('Linear'))\n",
    "    test.assertEqual(num_linear, 3)\n",
    "\n",
    "    # Test MLP gradients\n",
    "    # Test forward pass\n",
    "    x_test = torch.randn(N, in_features)\n",
    "    labels = torch.randint(low=0, high=num_classes, size=(N,), dtype=torch.long)\n",
    "    z = mlp(x_test)\n",
    "    test.assertSequenceEqual(z.shape, [N, num_classes])\n",
    "\n",
    "    # Create a sequence of MLPs and CE loss\n",
    "    seq_mlp = layers.Sequential(mlp, layers.CrossEntropyLoss())\n",
    "    loss = seq_mlp(x_test, y=labels)\n",
    "    test.assertEqual(loss.dim(), 0)\n",
    "    print(f'MLP loss={loss}, activation={activation}')\n",
    "\n",
    "    # Test backward pass\n",
    "    test_block_grad(seq_mlp, x_test, y=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If the above tests passed then congratulations - you've now implemented an arbitrarily deep model and loss function with end-to-end automatic differentiation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Questions\n",
    "<a id=part2_7></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO** Answer the following questions. Write your answers in the appropriate variables in the module `hw2/answers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T21:11:02.340792Z",
     "iopub.status.busy": "2022-04-28T21:11:02.340682Z",
     "iopub.status.idle": "2022-04-28T21:11:02.356591Z",
     "shell.execute_reply": "2022-04-28T21:11:02.356291Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cs236781.answers import display_answer\n",
    "import hw2.answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Question 1 \n",
    "\n",
    "Suppose we have a linear (i.e. fully-connected) layer with a weight tensor $\\mat{W}$, defined with `in_features=1024` and `out_features=512`. We apply this layer to an input tensor $\\mat{X}$ containing a batch of `N=64` samples. The output of the layer is denoted as $\\mat{Y}$.\n",
    "\n",
    "1. Consider the Jacobian tensor $\\pderiv{\\mat{Y}}{\\mat{X}}$ of the output of the layer w.r.t. the input $\\mat{X}$.\n",
    "    1. What is the shape of this tensor?\n",
    "    1. Is this Jacobian sparse (most elements zero by definition)? If so, why and which elements?\n",
    "    1. Given the gradient of the output w.r.t. some downstream scalar loss $L$, $\\delta\\mat{Y}=\\pderiv{L}{\\mat{Y}}$, do we need to materialize the above Jacobian in order to calculate the downstream gratdient w.r.t. to the input ($\\delta\\mat{X}$)? If yes, explain why; if no, show how to calcualte it without materializing the Jacobian.\n",
    "\n",
    "1. Consider the Jacobian tensor $\\pderiv{\\mat{Y}}{\\mat{W}}$ of the output of the layer w.r.t. the layer weights $\\mat{W}$. Answer questions A-C about it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T21:11:02.358510Z",
     "iopub.status.busy": "2022-04-28T21:11:02.358403Z",
     "iopub.status.idle": "2022-04-28T21:11:02.375534Z",
     "shell.execute_reply": "2022-04-28T21:11:02.375256Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_answer(hw2.answers.part1_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Question 2\n",
    "\n",
    "Is back-propagation **required** in order to train neural networks with decent-based optimization? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T21:11:02.377445Z",
     "iopub.status.busy": "2022-04-28T21:11:02.377339Z",
     "iopub.status.idle": "2022-04-28T21:11:02.391686Z",
     "shell.execute_reply": "2022-04-28T21:11:02.391388Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_answer(hw2.answers.part1_q2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
